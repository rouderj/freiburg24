---
title: "Lecture 2: Posteriors"
author: Jeff Rouder
date: May, 2024

output:
  beamer_presentation:
    theme: "metropolis"
    fonttheme: "structurebold"
    fig_caption: false
    incremental: false
---

```{r,echo=F,eval=T,warning=F,message=F}
knitr::opts_chunk$set(echo = FALSE,message=FALSE, warning=FALSE)
```

## What Makes A Bayesian A Bayesian

- Freuqentists use probability on data (observations) but not on models or parameters

- Bayesians use probability on everything.  Models, parameters, data, etc.

- Bayes rule is the Law of Conditional Probability applied to parameters and models and data.


## Motivating Problem

What is the probability that toast falls butter side down?  Let's suppose we have observed 7 successes (butter-side down) in 10 trials.  What does that tell us about buttered toast?

## The Bayesian Specification

\[
Y|\theta \sim \mbox{Binomial}(\theta,n)
\]
The data are stated conditional on parameters rather than as a function of parameters.  

It is also clear this is incomplete.  Someone has to tell us which  $\theta$.


## The Bayesian Specification

In Bayesian analysis, we must *complete* the specification.  
\[
\begin{aligned}
Y|\theta &\sim \mbox{Binomial}(\theta,n)\\
\theta & \sim \mbox{Some Distribution}
\end{aligned}
\]


## Binomial Distribution on Data

Let's look at some predictions of the model for known $\theta$.  Here is $\theta=.7$:

```{r}
par(cex=1.5)
n=10
y=0:n
p=dbinom(y,n,.7)
plot(y,p,typ='h')
points(y,p,pch=19,col='red')
```

## Notation

The probability mass function (classical) is 
\[
Pr(Y=y;\theta) = \binom{n}{y}\theta^y(1-\theta)^{n-y}.
\]

Same function, Bayesian, is 
\[
Pr(Y=y|\theta) =  \binom{n}{y}\theta^y(1-\theta)^{n-y}.
\]

## Bayesian Analysis


Bayes' Rule for this case:
\[
f(\theta|y) = \frac{Pr(Y=y|\theta)}{Pr(Y=y)} \times f(\theta)
\]

1. $f(\theta|y)$, posterior distribution of parameter.  Beliefs after (or conditional on) seeing the data.

2. $f(\theta)$, prior or marginal distribution of parameters.  Our beliefs before seeing the data.  Flat, $\theta \sim \mbox{Uniform}(0,1)$.

3. $Pr(Y=y|\theta)$, conditional probability of observed data.  Known from conditional specification of model: 
\[
Pr(Y=y|\theta) = \binom{n}{y}\theta^y(1-\theta)^{n-y}
\]

4. $Pr(Y=y)$.  Marginal probability of data.  Uniquely Bayesian quantity without a frequentist analog.  


## $Pr(Y=y)$

Law of Total Probability (continuous form)

\[
Pr(Y=y) = \int_0^1 Pr(Y=y|\theta)f(\theta)\; d\theta
\]

- It's just a single number
- Not a function of parameters
- Not important for estimation, will not compute


## 

Here is Bayes rule again:
\[
f(\theta|y) = \frac{Pr(Y=y|\theta)}{Pr(Y=y)}\times f(\theta)
\]

or

\[
f(\theta|y) \propto Pr(Y=y|\theta)\times f(\theta)
\]

We are interested in the posterior, $f(\theta|y)$, which is a function of $\theta$ for fixed $y$.  So we can treat the RHS as a function of $\theta$ too.



## 

Previous. Probability density, $Pr(Y=y|\theta)$, is treated as function of $y$
```{r}
par(cex=1.5)
#usual, function of y for fixed theta
y=0:10
n=10
theta=.7
plot(y,dbinom(y,n,theta),typ='h')
text(2,.2,expression(theta==.7))
```

## 

Likelihood. Probability density,$Pr(Y=y|\theta)$, may be treated as function of $\theta$

```{r}
par(cex=1.5)
#in Bayes rule, function of theta for fixed y
y=7
n=10
theta=seq(0,1,.01)
plot(theta,dbinom(y,n,theta),typ='l',
     ylab="Likelihood",xlab=expression(theta))
text(.1,.2,"7 out of 10")
```

## Computational Form of Bayes Rule

\[
f(\theta|y) \propto L(\theta,y)\times f(\theta)
\]

"Posterior is proportional to the likelihood times the prior"


- Only good for surface understanding of Bayesian analysis.

- Misses the most important elements of Bayesian analysis, but that is for another time.

- Works for parameter estimation

## Likelihood

For binomial and observed data, 
\[
L(\theta;y,N) =  Pr(Y=y|\theta) = \binom{n}{y}\theta^y(1-\theta)^{n-y}
\]


## Let's Do It
\[ \begin{aligned}
f(\theta|y) &\propto L(\theta,y) \times f(\theta)\\
&\propto \binom{n}{y}\theta^y(1-\theta)^{n-y}\\
&\propto \theta^y(1-\theta)^{n-y}
\end{aligned}
\]

Hold this thought.

## Darn, Now What?

- Meet the *beta distribution*
- Flexible Form That Lives on [0,1]
- Good for propability parameters, such as $\theta$
- Two parameters determine the shape
- beta(1,1) is uniform



## Your Turn

Play with the following code, what do parameters $a$ and $b$ do? Obey $a>0$ and $b>0$.

```{r,echo=T,eval=F}
a=1
b=1
p=seq(0,1,.001)
plot(p,dbeta(p,a,b),typ='l')
```


## Density of a Beta

\[
\theta|a,b \sim \mbox{Beta}(a,b)
\]

implies

\[
f(\theta;a,b) \propto \theta^{a-1}(1-\theta)^{b-1}
\]

## Beta to the rescue

From before:
\[ 
f(\theta|y) \propto \theta^{y}(1-\theta)^{n-y}
\]

Almost a beta (missing the -1 in the exponents).  Here is beta:
\[
f(\theta|y) \propto \theta^{(y+1)-1}(1-\theta)^{(n-y+1)-1}
\]

\[
\theta|y \sim \mbox{Beta}(y+1,\;n-y+1)
\]


## 

```{r,echo=F}
par(cex=1.5)
y=7
n=10
a=1
b=1
theta=seq(0,1,.01)
post=dbeta(theta,y+a,n-y+b)
prior=dbeta(theta,a,b)
plot(theta,post,typ='l',ylab="Density",xlab=expression(theta),main="Posterior & Prior for 7 successes of 10 trials")
lines(theta,prior)
```

## Encoding Other Beliefs

```{r}
#Not very likely prior
par(cex=1.5)
theta=seq(0,1,.001)
plot(typ='l',theta,dbeta(theta,1,3),col='darkgreen',
     xlab=expression(theta),ylab="Prior Density")
lines(theta,dbeta(theta,2,2),col='darkblue')
lines(theta,dbeta(theta,3,1),col='darkred')
```

##

\[ \begin{aligned}
f(\theta|y) &\propto L(\theta,y) \times f(\theta)\\
&\propto \theta^y(1-\theta)^{n-y} \times \theta^{a-1}(1-\theta)^{b-1}\\
&\propto \theta^{y+a-1}(1-\theta)^{n-y+b-1}
\end{aligned}
\]
\[
\theta|y \sim \mbox{Beta}(y+a,n-y+b)
\]

## Encoding Other Beliefs

```{r}
#Not very likely prior
par(cex=1.5)
theta=seq(0,1,.001)
plot(typ='l',theta,dbeta(theta,1+7,3+3),col='darkgreen',
     xlab=expression(theta),ylab="Posterior Density", ylim=c(0,4))
lines(theta,dbeta(theta,2+7,2+3),col='darkblue')
lines(theta,dbeta(theta,3+7,1+3),col='darkred')
```



## Your Turn

Adapt my previous example with the uniform prior for the beta prior.  Consider two radically different priors, say Beta(10,.5) and Beta(.5,10).  What is the effect for 7 successes out of 10 flips?  How about 70 out of 100?  700 out of 1000?  For estimation, the effect of the prior seemingly fades as the sample size gets larget (often but not always true).



## Analysis of the Normal

The normal distribution is quite flexible, popular, and convenient.  It is fairly ubiquitous and understanding how to analyze it is essential.  The goal here is to discuss how to do so.  

Let $Y_1,Y_2,\ldots,Y_N$ be a sequence of $N$ random variables.  The normal model is given by
\[
Y_i | \mu,\sigma^2 \sim \mbox{Normal}(\mu,\sigma^2),\quad i=1,\ldots,N
\]
where the normal is a two-parameter symmetric distribution with parameters $\mu$ and $\sigma^2$ and density given by: 
\[f(y) = f(y|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)
\]


## Posterior of $\mu$ for Known $\sigma^2$

The goal here is to estimate $\mu$ conditional on the observed data $y_1,\ldots,y_N$.  

1. We need prior beliefs on $\mu$, let's use $\mu \sim \mbox{Normal}(a,b)$.  Setting $a$ is prior meanl setting $b$ is prior variance.  These values of $a$ and $b$ are set before hand.

2. To update our beliefs we use, wait for it......, Bayes Rule:
\[
f(\mu|\sigma^2,y_1,\ldots,y_N) \propto f(y_1,\ldots,y_N|\mu,\sigma^2) \times f(\mu|\sigma^2)
\]
The term $f(\mu|\sigma^2,y_1,\ldots,y_N)$ is called the conditional posterior distribution of $\mu$.

## Posterior of $\mu$ for Known $\sigma^2$

3. The prior on $\mu$ holds for any $\sigma^2$, hence, 
\[
f(\mu|\sigma^2)=f(\mu) = \frac{1}{\sqrt{2\pi b}} \exp\left(-\frac{(\mu-a)^2}{2 b}\right).
\]

## Posterior of $\mu$ for Known $\sigma^2$

4. The remaining part is $f(y_1,\ldots,y_N|\mu,\sigma^2)$.  If there was one piece of datum, $y_1$, the density is
\[
f(y_1|\mu,\sigma^2)  = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_1-\mu)^2}{2\sigma^2}\right),
\]
If there were two observations, $y_1$ and $y_2$, then
\[ \begin{aligned}
f(y_1,y_2|\mu,\sigma^2) = &\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_1-\mu)^2}{2\sigma^2}\right)\\ &\times \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_2-\mu)^2}{2\sigma^2}\right). \end{aligned}
\]

## Posterior of $\mu$ for Known $\sigma^2$

Collecting terms yields:
\[
f(y_1,y_2|\mu,\sigma^2) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{(y_1-\mu)^2+(y_2-\mu)^2}{2\sigma^2}\right).\]
So, for all observations:

\[
f(y_1,\ldots,y_N|\mu,\sigma^2) = \frac{1}{(2\pi)^{N/2}(\sigma^2)^{N/2}}\exp\left(-\frac{\sum_i(y_i-\mu)^2}{2\sigma^2}\right).
\]


## Posterior of $\mu$ for Known $\sigma^2$

5. Putting it altogether:
\[ \begin{aligned}
f(\mu|\sigma^2,y_1,\ldots,y_N) \propto & f(y_1,\ldots,y_N|\mu,\sigma^2) \times f(\mu|\sigma^2)\\
f(\mu|\sigma^2,y_1,\ldots,y_N) \propto & \frac{1}{(2\pi)^{N/2}(\sigma^2)^{N/2}}\exp\left(-\frac{\sum_i(y_i-\mu)^2}{2\sigma^2}\right) \times\\
&\frac{1}{\sqrt{2\pi b}} \exp\left(-\frac{(\mu-a)^2}{2 b}\right).
\end{aligned}
\]


## Posterior of $\mu$ for Known $\sigma^2$

6. The above may be reduced (a small miracle occurs here, see Rouder and Lu, 2005) to
\[
f(\mu|\sigma^2,y_1,\ldots,y_N) \propto \frac{1}{\sqrt{2\pi v}}\exp\left(-\frac{(y-cv)^2}{2v}\right),
\]
where
\[v=\left(\frac{N}{\sigma^2}+\frac{1}{b}\right)^{-1}\]
and 
\[
c=\left(\frac{N\bar{y}}{\sigma^2}+\frac{a}{b}\right)\]

You may notice that this is the density of a normal with mean $vc$ and variance $v$, e.g.,

\[
\mu|\sigma^2;y_1,\ldots,y_N \sim \mbox{Normal}(cv,v).
\]


## Your Turn

Plot the prior and posterior for

- $N=50$
- $\bar{y}=550$
- $a=700$
- $b=300^2$
- $\sigma^2=200^2$

## Example, Smarties:

Suppose we wished to know the effects of “Smarties,” a brand of candy, on IQ. Certain children have been known to implore their parents for Smarties with the claim that it assuredly makes them smarter. Let’s assume for argument’s sake that we have fed Smarties to a randomly selected group of school children, and then measured their IQ, which we model as a normal.  Let's assume the test we used has been normed so that $\sigma^2=15^2=225$.  

## Data

- $N=10$ and 
- $\bar{y}=95$.  

## Frequentist Analysis, Mean + 95%CI

```{r}
par(cex=1.5)
a=100
b=2^2
ybar=95
N=10
s2=15^2

c=((N*ybar/s2)+(a/b))
v=1/((N/s2)+(1/b))

IQ=seq(80,120,.05)
#posterior
plot(IQ,dnorm(IQ,c*v,sqrt(v)),typ='n',axes=F,ylab="")
#lines(IQ,dnorm(IQ,a,sqrt(b)),col='darkblue')
ci.width=1.96*sqrt(s2/N)
my.y=.10
arrows(ybar-ci.width,my.y,ybar+ci.width,my.y,angle=90,lwd=2,code=3)
points(ybar,my.y,pch=19,cex=1.2)
axis(1)
```


## Bayesian Analysis, Prior + Posterior

I am pretty sure that even if there is an effect, it will be small, say on the order of 2 IQ points. So I am going to choose $a=100$ and $b=2^2$ as a prior.  That means true IQ can reasonable vary between say 96 and 104 ($\pm$ 2 standard deviations).

## Bayesian Analysis, Prior + Posterior

```{r}
par(cex=1.5)
a=100
b=2^2
ybar=95
N=10
s2=15^2

c=((N*ybar/s2)+(a/b))
v=1/((N/s2)+(1/b))

IQ=seq(80,120,.05)
#posterior
plot(IQ,dnorm(IQ,c*v,sqrt(v)),typ='l',axes=F,ylab="")
lines(IQ,dnorm(IQ,a,sqrt(b)),col='darkblue')
ci.width=1.96*sqrt(s2/N)
my.y=.10
arrows(ybar-ci.width,my.y,ybar+ci.width,my.y,
       angle=90,lwd=1,code=3,lty=2)
points(ybar,my.y,pch=19,cex=1.2)
axis(1)
```


# Exponential Data, Gamma Prior

\[
\begin{aligned}
Y_i|\lambda &\sim \mbox{Exponential}(\lambda), \quad i=1,\ldots,n\\
\lambda &\sim \mbox{Gamma}(a,b)
\end{aligned}
\]

* Use wiki to get acquainted with the exponential and gamma.  Here,  $\lambda$ is a rate parameter.  What are the roles of $(a,b)$, how should we set them?

* Using the proportional form of Bayes' Rule, derive the posterior $\lambda|Y$.


## Suppose both $\mu$ and $\sigma^2$ are unknown


\[
\begin{aligned}
Y_i &\sim \mbox{Normal}(\mu,\sigma^2)\\
\mu &\sim \mbox{Normal}(a,b)\\
\sigma^2 &\sim \mbox{Imverse Gamma}(q,s)
\end{aligned}
\]


## Your Turn

What is an inverse gamma distribution?  

Use Wiki

## Suppose both $\mu$ and $\sigma^2$ are unknown


We can still use, wait for it,  Bayes rule:
\[
f(\mu,\sigma^2|y_1,\ldots,y_N) \propto f(y_1,\ldots,y_N|\mu,\sigma^2) \times f(\mu,\sigma^2)
\]


We are just going to do the rest in R rather than in math.  So, lets do IQ with observations (105,100,124,104)

## Joint Prior over $\mu$ and $\sigma^2$

```{r}
par(cex=1.5)
library(MCMCpack)
#Show prior
a=100
b=5^2
r=1
s=10^2

mu=seq(80,120,1)
s2=seq(1^2,20^2,1)

my.prior=function(mu,s2) dnorm(mu,a,sqrt(b))*dinvgamma(s2,r,s)
prior=outer(mu,s2,my.prior)
contour(mu,s2,prior,xlab=expression(mu),ylab=expression(sigma^2))
```

## Likelihood Over $\mu$ and $\sigma^2$ for observations

```{r}
par(cex=1.5)
#Show likelihood
dat=c(105,100,124,104)
my.like=function(mu,s2) exp(sum(dnorm(log=T,dat,mu,sqrt(s2))))
my.like=Vectorize(my.like)
like=outer(mu,s2,my.like)
contour(mu,s2,like,xlab=expression(mu),ylab=expression(sigma^2))
```

## Joint Posterior over $\mu$ and $\sigma^2$ 

```{r}
par(cex=1.5)
#posterior
post=prior*like
total=sum(post)
post=post/total #sums to 1
contour(mu,s2,post,xlab=expression(mu),ylab=expression(sigma^2))
```

## Marginal Posterior for $\mu$

```{r}
par(cex=1.5)
#Marginal Distributions
mu.post=apply(post,1,sum)
plot(mu,mu.post,typ='l',xlab=expression(mu),ylab="Posterior Density")
```

## Marginal Posterior for $\sigma^2$

```{r}
par(cex=1.5)
s2.post=apply(post,2,sum)
plot(s2,s2.post,typ='l',xlab=expression(sigma^2),ylab="Posterior Density")
```

