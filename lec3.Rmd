---
title: "Introduction To Sampling"
author: Jeff Rouder
date:  May, 2024

output:
  beamer_presentation:
    theme: "metropolis"
    fonttheme: "structurebold"
    fig_caption: false
    incremental: false
    
header-includes   :
   - \usepackage{bm}
   - \usepackage{pcl}
   - \usepackage{amsmath}    
---

```{r,echo=F,eval=T,warning=F,message=F}
knitr::opts_chunk$set(echo = FALSE,message=FALSE, warning=FALSE)
set.seed(1256)
library(MCMCpack)
```

## Estimate the parameters in a normal

- I mean, how hard is this
- $\hat{\mu} = \frac{\sum y_i}{n}$
- $\hat{\sigma^2} =\frac{\sum(y_i-\bar{y})^2)}{n-1}$
- $\mbox{CI}=\pm t(n-1,.975)\times\sqrt{n}\times\frac{\hat{\mu}}{\hat{\sigma}}$

## Bayesian...

```{r}
par(cex=1.5)
lab=c("Easy","Hard","Impossible")
x=1:3
y=rep(2,3)
plot(x,y,axes=F,ylim=c(1,3),typ='l',ylab="Bayes",xlab="Frequentist")
axis(1,at=1:3,lab=lab)
axis(2,at=1:3,lab=lab)

```



## Sampling as Knowledge

Who Wakes Up First?

Let $X$ denote when Becky wakes up.  It is minutes after 7a.

\[
X \sim \mbox{Gamma}(2,10)
\]

Let $Y$ denote when Jeff wakes up.  It is minutes after 6:30a.

\[
Y \sim \mbox{Gamma}(1,30)
\]

What is the probability Becky wakes up before Jeff, $Pr(X<Y)$?  


## Setup

```{r,echo=F}
par(cex=1.5)
t=seq(0,120,.1)
jeff=dgamma(t,shape=1,scale=30)
becky=dgamma(t-30,shape=2,scale=10)
plot(t,becky,typ='l',ylab="Density",xlab="Time after 6:30a (minutes)")
lines(t,jeff)
```

## Simulation Code

```{r,eval=F,echo=T}
M=100000
j=rgamma(M,shape=1,scale=30)
b=rgamma(M,shape=2,scale=10)+30
hist(b,breaks=seq(0,500,2),col=rgb(1,0,0,.5),
     xlim=c(0,180),prob=T)
hist(j,add=T,col=rgb(0,1,0,.5),breaks=seq(0,500,2),prob=T)
print(mean(b<j))
```
  
## Simulations

```{r,echo=F,eval=T}
M=100000
j=rgamma(M,shape=1,scale=30)
b=rgamma(M,shape=2,scale=10)+30
hist(b,breaks=seq(0,500,2),col=rgb(1,0,0,.5),xlim=c(0,180),prob=T)
hist(j,add=T,col=rgb(0,1,0,.5),breaks=seq(0,500,2),prob=T)
print(mean(b<j))
```
  
## MCMC techniques

- Samples from posterior
- e.g. $(\mu,\sigma^2|\bfY)$
- Samples are enough to learn

## How To?

- Program Your Own Sampler To Learn About MCMC
  + fast to execute
  + you feel smart
  + insightful
- JAGS
  + fast but autocorrelation
  + great manual
  + stable
- stan
  + best samplers, 
  + changing, info scattered
  + very popular (cool kids)


## Model + Data

\[
\begin{aligned}
Y_i \sim& \mbox{N}(\mu,\sigma^2)\\
\mu \sim & \mbox{N}(a,b)\\
\sigma^2 \sim & \mbox{Inverse Gamma}(q,r)
\end{aligned}
\]

$Y$= 92, 115, 100, 98


## Demo, Roll My Own

\[
\mu \mid \sigma^2,\bfY \sim \mbox{Normal}(vc,v),\quad v=\left(\frac{N}{\sigma^2}+\frac{1}{b}\right)^{-1}, \quad c=\frac{N\bar{Y}}{\sigma^2}+\frac{a}{b}
\]

\[
\sigma^2 \mid \mu, \bfY \sim \mbox{IG}\left(q+\frac{N}{2},r+\frac{SSE}{2}\right)
\]
]

## Demo, Roll My Own

```{r,echo=T,include=F}
library(MCMCpack)
```

```{r,echo=T}
Y=c(92,115,100,98)
ybar=mean(Y)
N=length(Y)
a=100
b=25
q=.5
r=.5*15^2
M=20000 # number of samples
mu=1:M
s2=1:M
mu[1]=100 #initial value
s2[1]=100 #initial value
```

## Demo, Roll My Own
```{r,echo=T}
for (m in 2:M){
  v=1/(N/s2[m-1]+1/b)
  c=N*ybar/s2[m-1]+a/b
  mu[m]=rnorm(1,c*v,sqrt(v))
  SSE=sum((Y-mu[m])^2)
  s2[m]=rinvgamma(1,N/2+q,SSE/2+r)
  }
```
  
## Demo Roll My Own
```{r}
par(cex=1.5)
par(mfrow=c(1,2),mar=c(4,4,2,2))
plot(mu,typ='l')
plot(sqrt(s2),typ='l')
```

## Demo Roll My Own
```{r}
par(cex=1.5)
par(mfrow=c(1,2),mar=c(4,4,2,2))
hist(mu,breaks=50)
hist(sqrt(s2),breaks=50)
```

## JAGS & stan

- Examples in their directories
- It is too hard to typeset here




## Two-sample analysis

- Half the kids get Smarties (treatment)
- Half don't (control)
- All get an IQ test an hour later
- What was the effect of Smarties?
- You make up the data to explore the following questions
- Let $Y_{ij}$ denote the IQ for the $j$th kiddo, $j=1,\ldots,J$ in the $i$th group, $i=1,2$. ($i=1$ for control, $i=2$ for treatment)
- $Y_1$= 101,  82, 136,  77, 102,  99
- $Y_2$= 104, 101, 115,  88,  83, 122


## Two Models, Which Is Better?

Cell Means Parameterization:
\[ \begin{aligned}
Y_{1j} &\sim \mbox{N}(\mu_1,\sigma^2)\\
Y_{2j} &\sim \mbox{N}(\mu_2,\sigma^2)
\end{aligned} \]

Same, but more compact:
\[
Y_{ij} \sim \mbox{N}(\mu_i,\sigma^2)
\]

Effects Parameterization:
\[ \begin{aligned}
Y_{1j} &\sim \mbox{N}(\alpha-\theta/2,\sigma^2)\\
Y_{2j} &\sim \mbox{N}(\alpha+\theta/2,\sigma^2)
\end{aligned} \]

Same but more compact:
\[
Y_{ij} \sim \mbox{N}(\alpha+x_i\theta,\sigma^2), \quad x_i=c(-1/2,1/2)\]


## Two Models, Which Is Better?

Cell Means:
\[ \begin{aligned}
Y_{ij} &\sim \mbox{N}(\mu_i,\sigma^2)\\
\mu_i &\sim \mbox{N}(a,b)\\
\sigma^2 &\sim \mbox{IG}(1/2,r/2)
\end{aligned}
\]

Effects:
\[ \begin{aligned}
Y_{ij} &\sim \mbox{N}(\alpha+x_i\theta,\sigma^2)\\
\alpha &\sim \mbox{N}(a_1,b_1)\\
\theta &\sim \mbox{N}(a_2,b_2)\\
\sigma^2 &\sim \mbox{IG}(1/2,r/2)
\end{aligned}
\]

- $a=a_1=100$, $b=b_1=5^2$, $r=15^2$
- $a_2=0$, $b_2=1^2$

## Your Turn

- use JAGS or stan
- program up both models
- does it matter?
- write a note to your colleague explaining why effects models are preferred.

